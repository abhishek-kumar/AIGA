Once the job(query) is submitted, the Manager queues it for future consideration by the scheduler. 

The standard Alchemi scheduler queries the status of each executor (and probly also queries the lookup table to determine the subset of executors to query; or each executor may have a partial lookup table for hierarchical probing...lol im getting insane lets keep it simple) and finally dispatches the job to the available one (this is in dedicated mode. In the non dedicated mode, exactly the opposite happens: the executors query the manager for jobs whenever they get free and pull the tasks from the queue). 

We propose an alternate scheduling algorithm implemented via the GRidbus broker[refernce no here]. Once the jobs are queued up, our scheduler picks up each query one at a time and runs it through an NLP parser which returns with a generalized set of keywords (our NLP parser includes support for synonyms so that every new word doesnt map to a new domain, but rather simplifies to an existing domain. This makes it possible to have a one-many mapping between domains and keywords). These keywords are then compared with a lookup table to determine the subset of executors (knowledge farm) to query. Once the Subset is determined, scheduler queries the status of each executor and finally dispatches the job to the available one. Note that for this model to work successfully, data redundancy is a necessity (without which each node would have unique data and so each query would have to be mapped to one and only one node - load balancing goes out of question). Another aspect that this presents is controlled redundance - redundancy of frequently used information, which includes performance and reduces wastage.